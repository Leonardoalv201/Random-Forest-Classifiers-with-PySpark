{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e15c57-7894-4b07-8325-0af5af4f5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lalvaradomenendez\\appdata\\local\\anaconda3\\envs\\spark-env\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lalvaradomenendez\\appdata\\local\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lalvaradomenendez\\appdata\\local\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lalvaradomenendez\\appdata\\local\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\lalvaradomenendez\\appdata\\local\\anaconda3\\envs\\spark-env\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lalvaradomenendez\\appdata\\local\\anaconda3\\envs\\spark-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74900eef-9239-437b-b463-3ca827eceec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c45471-9313-4a46-bf2b-c0300970b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # entry point for pyspark\n",
    "\n",
    "# instantiate spark instance\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Random Forest eCommerce\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2545a1d-8574-44da-a92d-5a57a32fa426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"2019-Nov.csv\"  # wherever path you saved the kaggle file to\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "df.printSchema()  # to see the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78aab01a-b371-496d-884c-2b3628b709ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>event_time</th>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>2019-11-01 00:00:00 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:01 UTC</td>\n",
       "      <td>2019-11-01 00:00:02 UTC</td>\n",
       "      <td>2019-11-01 00:00:02 UTC</td>\n",
       "      <td>2019-11-01 00:00:02 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <td>1003461</td>\n",
       "      <td>5000088</td>\n",
       "      <td>17302664</td>\n",
       "      <td>3601530</td>\n",
       "      <td>1004775</td>\n",
       "      <td>1306894</td>\n",
       "      <td>1306421</td>\n",
       "      <td>15900065</td>\n",
       "      <td>12708937</td>\n",
       "      <td>1004258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_id</th>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>2053013566100866035</td>\n",
       "      <td>2053013553853497655</td>\n",
       "      <td>2053013563810775923</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>2053013558190408249</td>\n",
       "      <td>2053013553559896355</td>\n",
       "      <td>2053013555631882655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_code</th>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>appliances.sewing_machine</td>\n",
       "      <td>None</td>\n",
       "      <td>appliances.kitchen.washer</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <td>xiaomi</td>\n",
       "      <td>janome</td>\n",
       "      <td>creed</td>\n",
       "      <td>lg</td>\n",
       "      <td>xiaomi</td>\n",
       "      <td>hp</td>\n",
       "      <td>hp</td>\n",
       "      <td>rondell</td>\n",
       "      <td>michelin</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>489.07</td>\n",
       "      <td>293.65</td>\n",
       "      <td>28.31</td>\n",
       "      <td>712.87</td>\n",
       "      <td>183.27</td>\n",
       "      <td>360.09</td>\n",
       "      <td>514.56</td>\n",
       "      <td>30.86</td>\n",
       "      <td>72.72</td>\n",
       "      <td>732.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>520088904</td>\n",
       "      <td>530496790</td>\n",
       "      <td>561587266</td>\n",
       "      <td>518085591</td>\n",
       "      <td>558856683</td>\n",
       "      <td>520772685</td>\n",
       "      <td>514028527</td>\n",
       "      <td>518574284</td>\n",
       "      <td>532364121</td>\n",
       "      <td>532647354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_session</th>\n",
       "      <td>4d3b30da-a5e4-49df-b1a8-ba5943f1dd33</td>\n",
       "      <td>8e5f4f83-366c-4f70-860e-ca7417414283</td>\n",
       "      <td>755422e7-9040-477b-9bd2-6a6e8fd97387</td>\n",
       "      <td>3bfb58cd-7892-48cc-8020-2f17e6de6e7f</td>\n",
       "      <td>313628f1-68b8-460d-84f6-cec7a8796ef2</td>\n",
       "      <td>816a59f3-f5ae-4ccd-9b23-82aa8c23d33c</td>\n",
       "      <td>df8184cc-3694-4549-8c8c-6b5171877376</td>\n",
       "      <td>5e6ef132-4d7c-4730-8c7f-85aa4082588f</td>\n",
       "      <td>0a899268-31eb-46de-898d-09b2da950b24</td>\n",
       "      <td>d2d3d2c6-631d-489e-9fb5-06f340b85be0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0  \\\n",
       "event_time                  2019-11-01 00:00:00 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1003461   \n",
       "category_id                     2053013555631882655   \n",
       "category_code                electronics.smartphone   \n",
       "brand                                        xiaomi   \n",
       "price                                        489.07   \n",
       "user_id                                   520088904   \n",
       "user_session   4d3b30da-a5e4-49df-b1a8-ba5943f1dd33   \n",
       "\n",
       "                                                  1  \\\n",
       "event_time                  2019-11-01 00:00:00 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  5000088   \n",
       "category_id                     2053013566100866035   \n",
       "category_code             appliances.sewing_machine   \n",
       "brand                                        janome   \n",
       "price                                        293.65   \n",
       "user_id                                   530496790   \n",
       "user_session   8e5f4f83-366c-4f70-860e-ca7417414283   \n",
       "\n",
       "                                                  2  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                 17302664   \n",
       "category_id                     2053013553853497655   \n",
       "category_code                                  None   \n",
       "brand                                         creed   \n",
       "price                                         28.31   \n",
       "user_id                                   561587266   \n",
       "user_session   755422e7-9040-477b-9bd2-6a6e8fd97387   \n",
       "\n",
       "                                                  3  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  3601530   \n",
       "category_id                     2053013563810775923   \n",
       "category_code             appliances.kitchen.washer   \n",
       "brand                                            lg   \n",
       "price                                        712.87   \n",
       "user_id                                   518085591   \n",
       "user_session   3bfb58cd-7892-48cc-8020-2f17e6de6e7f   \n",
       "\n",
       "                                                  4  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1004775   \n",
       "category_id                     2053013555631882655   \n",
       "category_code                electronics.smartphone   \n",
       "brand                                        xiaomi   \n",
       "price                                        183.27   \n",
       "user_id                                   558856683   \n",
       "user_session   313628f1-68b8-460d-84f6-cec7a8796ef2   \n",
       "\n",
       "                                                  5  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1306894   \n",
       "category_id                     2053013558920217191   \n",
       "category_code                    computers.notebook   \n",
       "brand                                            hp   \n",
       "price                                        360.09   \n",
       "user_id                                   520772685   \n",
       "user_session   816a59f3-f5ae-4ccd-9b23-82aa8c23d33c   \n",
       "\n",
       "                                                  6  \\\n",
       "event_time                  2019-11-01 00:00:01 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                  1306421   \n",
       "category_id                     2053013558920217191   \n",
       "category_code                    computers.notebook   \n",
       "brand                                            hp   \n",
       "price                                        514.56   \n",
       "user_id                                   514028527   \n",
       "user_session   df8184cc-3694-4549-8c8c-6b5171877376   \n",
       "\n",
       "                                                  7  \\\n",
       "event_time                  2019-11-01 00:00:02 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                 15900065   \n",
       "category_id                     2053013558190408249   \n",
       "category_code                                  None   \n",
       "brand                                       rondell   \n",
       "price                                         30.86   \n",
       "user_id                                   518574284   \n",
       "user_session   5e6ef132-4d7c-4730-8c7f-85aa4082588f   \n",
       "\n",
       "                                                  8  \\\n",
       "event_time                  2019-11-01 00:00:02 UTC   \n",
       "event_type                                     view   \n",
       "product_id                                 12708937   \n",
       "category_id                     2053013553559896355   \n",
       "category_code                                  None   \n",
       "brand                                      michelin   \n",
       "price                                         72.72   \n",
       "user_id                                   532364121   \n",
       "user_session   0a899268-31eb-46de-898d-09b2da950b24   \n",
       "\n",
       "                                                  9  \n",
       "event_time                  2019-11-01 00:00:02 UTC  \n",
       "event_type                                     view  \n",
       "product_id                                  1004258  \n",
       "category_id                     2053013555631882655  \n",
       "category_code                electronics.smartphone  \n",
       "brand                                         apple  \n",
       "price                                        732.07  \n",
       "user_id                                   532647354  \n",
       "user_session   d2d3d2c6-631d-489e-9fb5-06f340b85be0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.take(10), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247b5a79-1cab-4e50-95c3-f250510ce0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT user_id)|\n",
      "+-----------------------+\n",
      "|                3696117|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using native pyspark\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"user_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe810e0b-74a1-44fe-9ea4-b2f1e66aba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event_time you should use a window and groupby a time period\n",
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baeb3e3a-a142-49ff-842b-c30ad337d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|            features|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+--------------------+\n",
      "|2019-11-01 00:00:...|      view|   1003461|2053013555631882655|electronics.smart...|  xiaomi|489.07|520088904|4d3b30da-a5e4-49d...|[1003461.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|   5000088|2053013566100866035|appliances.sewing...|  janome|293.65|530496790|8e5f4f83-366c-4f7...|[5000088.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|  17302664|2053013553853497655|                null|   creed| 28.31|561587266|755422e7-9040-477...|[1.7302664E7,2.05...|\n",
      "|2019-11-01 00:00:...|      view|   3601530|2053013563810775923|appliances.kitche...|      lg|712.87|518085591|3bfb58cd-7892-48c...|[3601530.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|   1004775|2053013555631882655|electronics.smart...|  xiaomi|183.27|558856683|313628f1-68b8-460...|[1004775.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|   1306894|2053013558920217191|  computers.notebook|      hp|360.09|520772685|816a59f3-f5ae-4cc...|[1306894.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|   1306421|2053013558920217191|  computers.notebook|      hp|514.56|514028527|df8184cc-3694-454...|[1306421.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|  15900065|2053013558190408249|                null| rondell| 30.86|518574284|5e6ef132-4d7c-473...|[1.5900065E7,2.05...|\n",
      "|2019-11-01 00:00:...|      view|  12708937|2053013553559896355|                null|michelin| 72.72|532364121|0a899268-31eb-46d...|[1.2708937E7,2.05...|\n",
      "|2019-11-01 00:00:...|      view|   1004258|2053013555631882655|electronics.smart...|   apple|732.07|532647354|d2d3d2c6-631d-489...|[1004258.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|  17200570|2053013559792632471|furniture.living_...|    null|437.33|518780843|aa806835-b14c-45a...|[1.720057E7,2.053...|\n",
      "|2019-11-01 00:00:...|      view|   2701517|2053013563911439225|appliances.kitche...|    null|155.11|518427361|c89b0d96-247f-404...|[2701517.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|  16700260|2053013559901684381|furniture.kitchen...|    null| 31.64|566255262|173d7b72-1db7-463...|[1.670026E7,2.053...|\n",
      "|2019-11-01 00:00:...|      view|  34600011|2060981320581906480|                null|    null| 20.54|512416379|4dfe2c67-e537-4dc...|[3.4600011E7,2.06...|\n",
      "|2019-11-01 00:00:...|      view|   4600658|2053013563944993659|appliances.kitche...| samsung|411.83|526595547|aab33a9a-29c3-4d5...|[4600658.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|  24900193|2053013562183385881|                null|    null|  1.09|512651494|f603c815-f51a-46f...|[2.4900193E7,2.05...|\n",
      "|2019-11-01 00:00:...|      view|  27400066|2053013563391345499|                null|    null|  8.55|551061950|3f6112f1-5695-4e8...|[2.7400066E7,2.05...|\n",
      "|2019-11-01 00:00:...|      view|   5100503|2053013553375346967|                null|  xiaomi| 22.68|520037415|f54fa96a-f3f2-43a...|[5100503.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|   1004566|2053013555631882655|electronics.smart...|  huawei|164.84|566265908|52c2c76c-b79e-479...|[1004566.0,2.0530...|\n",
      "|2019-11-01 00:00:...|      view|   1307115|2053013558920217191|  computers.notebook|      hp|411.59|514028527|df8184cc-3694-454...|[1307115.0,2.0530...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\n",
    "    \"product_id\",\n",
    "    \"category_id\",\n",
    "    \"price\"\n",
    "]  # columns you'd like to use\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e44ecc5-ee8b-49fd-836c-7ca97d0b7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+--------------------+-------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand| price|  user_id|        user_session|            features|encoded|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+--------------------+-------+\n",
      "|2019-11-01 00:00:...|      view|   1003461|2053013555631882655|electronics.smart...|  xiaomi|489.07|520088904|4d3b30da-a5e4-49d...|[1003461.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   5000088|2053013566100866035|appliances.sewing...|  janome|293.65|530496790|8e5f4f83-366c-4f7...|[5000088.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  17302664|2053013553853497655|                null|   creed| 28.31|561587266|755422e7-9040-477...|[1.7302664E7,2.05...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   3601530|2053013563810775923|appliances.kitche...|      lg|712.87|518085591|3bfb58cd-7892-48c...|[3601530.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   1004775|2053013555631882655|electronics.smart...|  xiaomi|183.27|558856683|313628f1-68b8-460...|[1004775.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   1306894|2053013558920217191|  computers.notebook|      hp|360.09|520772685|816a59f3-f5ae-4cc...|[1306894.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   1306421|2053013558920217191|  computers.notebook|      hp|514.56|514028527|df8184cc-3694-454...|[1306421.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  15900065|2053013558190408249|                null| rondell| 30.86|518574284|5e6ef132-4d7c-473...|[1.5900065E7,2.05...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  12708937|2053013553559896355|                null|michelin| 72.72|532364121|0a899268-31eb-46d...|[1.2708937E7,2.05...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   1004258|2053013555631882655|electronics.smart...|   apple|732.07|532647354|d2d3d2c6-631d-489...|[1004258.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  17200570|2053013559792632471|furniture.living_...|    null|437.33|518780843|aa806835-b14c-45a...|[1.720057E7,2.053...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   2701517|2053013563911439225|appliances.kitche...|    null|155.11|518427361|c89b0d96-247f-404...|[2701517.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  16700260|2053013559901684381|furniture.kitchen...|    null| 31.64|566255262|173d7b72-1db7-463...|[1.670026E7,2.053...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  34600011|2060981320581906480|                null|    null| 20.54|512416379|4dfe2c67-e537-4dc...|[3.4600011E7,2.06...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   4600658|2053013563944993659|appliances.kitche...| samsung|411.83|526595547|aab33a9a-29c3-4d5...|[4600658.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  24900193|2053013562183385881|                null|    null|  1.09|512651494|f603c815-f51a-46f...|[2.4900193E7,2.05...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|  27400066|2053013563391345499|                null|    null|  8.55|551061950|3f6112f1-5695-4e8...|[2.7400066E7,2.05...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   5100503|2053013553375346967|                null|  xiaomi| 22.68|520037415|f54fa96a-f3f2-43a...|[5100503.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   1004566|2053013555631882655|electronics.smart...|  huawei|164.84|566265908|52c2c76c-b79e-479...|[1004566.0,2.0530...|    0.0|\n",
      "|2019-11-01 00:00:...|      view|   1307115|2053013558920217191|  computers.notebook|      hp|411.59|514028527|df8184cc-3694-454...|[1307115.0,2.0530...|    0.0|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+------+---------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labeler = StringIndexer(\n",
    "    inputCol=\"event_type\", outputCol=\"encoded\"\n",
    ")  # what should we use for the inputCol here?\n",
    "df = labeler.fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3275f73-713d-42cf-b6dd-416ad19d3e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 47253900\n",
      "Test Dataset Count: 20248079\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac9158a6-3911-4230-b126-48f9deff6dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "||\n",
      "++\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"encoded\")\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "# what goes in the select() function?\n",
    "predictions.select(\"encoded\").show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb81dfe-0dd8-400f-bc2a-ebd2197d8da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9132064981073577\n",
      "Test Error = 0.08679350189264234\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"encoded\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a819e9e-4109-415a-bfc9-d242f8648e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 1641, USQROLALVARADO1.us.deloitte.com, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 585, in main\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 585, in main\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m preds_and_labels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      6\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(FloatType()))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m preds_and_labels \u001b[38;5;241m=\u001b[39m preds_and_labels\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mMulticlassMetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_and_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mconfusionMatrix()\u001b[38;5;241m.\u001b[39mtoArray())\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:256\u001b[0m, in \u001b[0;36mMulticlassMetrics.__init__\u001b[1;34m(self, predictionAndLabels)\u001b[0m\n\u001b[0;32m    254\u001b[0m sc \u001b[38;5;241m=\u001b[39m predictionAndLabels\u001b[38;5;241m.\u001b[39mctx\n\u001b[0;32m    255\u001b[0m sql_ctx \u001b[38;5;241m=\u001b[39m SQLContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m--> 256\u001b[0m numCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mpredictionAndLabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    257\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m    258\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    259\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)])\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numCol \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\rdd.py:1464\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1464\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   1466\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\rdd.py:1446\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1443\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1445\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1446\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   1449\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\context.py:1137\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m# Implementation note: This is implemented as a mapPartitions followed\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# by runJob() in order to avoid having to pass a Python lambda into\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# SparkContext#runJob.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m-> 1137\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\pyspark\\sql\\utils.py:131\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    133\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 1641, USQROLALVARADO1.us.deloitte.com, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 585, in main\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 585, in main\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 593, in read_int\n    length = stream.read(4)\n  File \"C:\\Users\\lalvaradomenendez\\AppData\\Local\\anaconda3\\envs\\spark-env\\lib\\socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nsocket.timeout: timed out\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2133)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = (\n",
    "    predictions.select([\"prediction\", \"encoded\"])\n",
    "    .withColumn(\"encoded\", F.col(\"encoded\").cast(FloatType()))\n",
    "    .orderBy(\"prediction\")\n",
    ")\n",
    "preds_and_labels = preds_and_labels.select([\"prediction\", \"encoded\"])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
